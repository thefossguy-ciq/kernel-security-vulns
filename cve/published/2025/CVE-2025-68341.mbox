From bippy-1.2.0 Mon Sep 17 00:00:00 2001
From: Greg Kroah-Hartman <gregkh@kernel.org>
To: <linux-cve-announce@vger.kernel.org>
Reply-to: <cve@kernel.org>, <linux-kernel@vger.kernel.org>
Subject: CVE-2025-68341: veth: reduce XDP no_direct return section to fix race
Message-Id: <2025122332-CVE-2025-68341-c207@gregkh>
Content-Length: 3158
Lines: 71
X-Developer-Signature: v=1; a=openpgp-sha256; l=3230;
 i=gregkh@linuxfoundation.org; h=from:subject:message-id;
 bh=eR/FolMy7aDGF4aavF0soYCQvkmC3xWRJuLyTtZyw6U=;
 b=owGbwMvMwCRo6H6F97bub03G02pJDJleCzj0uxxm5ugWL7kokRS2QcTXT+asVtObngt5Lb8fi
 e4XM2rqiGFhEGRikBVTZPmyjefo/opDil6Gtqdh5rAygQxh4OIUgIkE/2GYcfeIuPiZ9R82MR/K
 zbf83rb0opEew4LZRXFKmnFcZ+bYJd0WKJFXk/D4mw8A
X-Developer-Key: i=gregkh@linuxfoundation.org; a=openpgp;
 fpr=F4B60CC5BF78C2214A313DCB3147D40DDB2DFB29

Description
===========

In the Linux kernel, the following vulnerability has been resolved:

veth: reduce XDP no_direct return section to fix race

As explain in commit fa349e396e48 ("veth: Fix race with AF_XDP exposing
old or uninitialized descriptors") for veth there is a chance after
napi_complete_done() that another CPU can manage start another NAPI
instance running veth_pool(). For NAPI this is correctly handled as the
napi_schedule_prep() check will prevent multiple instances from getting
scheduled, but for the remaining code in veth_pool() this can run
concurrent with the newly started NAPI instance.

The problem/race is that xdp_clear_return_frame_no_direct() isn't
designed to be nested.

Prior to commit 401cb7dae813 ("net: Reference bpf_redirect_info via
task_struct on PREEMPT_RT.") the temporary BPF net context
bpf_redirect_info was stored per CPU, where this wasn't an issue. Since
this commit the BPF context is stored in 'current' task_struct. When
running veth in threaded-NAPI mode, then the kthread becomes the storage
area. Now a race exists between two concurrent veth_pool() function calls
one exiting NAPI and one running new NAPI, both using the same BPF net
context.

Race is when another CPU gets within the xdp_set_return_frame_no_direct()
section before exiting veth_pool() calls the clear-function
xdp_clear_return_frame_no_direct().

The Linux kernel CVE team has assigned CVE-2025-68341 to this issue.


Affected and fixed versions
===========================

	Issue introduced in 6.11 with commit 401cb7dae8130fd34eb84648e02ab4c506df7d5e and fixed in 6.12.61 with commit c1ceabcb347d1b0f7e70a7384ec7eff3847b7628
	Issue introduced in 6.11 with commit 401cb7dae8130fd34eb84648e02ab4c506df7d5e and fixed in 6.17.11 with commit d0bd018ad72a8a598ae709588934135017f8af52
	Issue introduced in 6.11 with commit 401cb7dae8130fd34eb84648e02ab4c506df7d5e and fixed in 6.18 with commit a14602fcae17a3f1cb8a8521bedf31728f9e7e39

Please see https://www.kernel.org for a full list of currently supported
kernel versions by the kernel community.

Unaffected versions might change over time as fixes are backported to
older supported kernel versions.  The official CVE entry at
	https://cve.org/CVERecord/?id=CVE-2025-68341
will be updated if fixes are backported, please check that for the most
up to date information about this issue.


Affected files
==============

The file(s) affected by this issue are:
	drivers/net/veth.c


Mitigation
==========

The Linux kernel CVE team recommends that you update to the latest
stable kernel version for this, and many other bugfixes.  Individual
changes are never tested alone, but rather are part of a larger kernel
release.  Cherry-picking individual commits is not recommended or
supported by the Linux kernel community at all.  If however, updating to
the latest release is impossible, the individual changes to resolve this
issue can be found at these commits:
	https://git.kernel.org/stable/c/c1ceabcb347d1b0f7e70a7384ec7eff3847b7628
	https://git.kernel.org/stable/c/d0bd018ad72a8a598ae709588934135017f8af52
	https://git.kernel.org/stable/c/a14602fcae17a3f1cb8a8521bedf31728f9e7e39
