From bippy-1.2.0 Mon Sep 17 00:00:00 2001
From: Greg Kroah-Hartman <gregkh@kernel.org>
To: <linux-cve-announce@vger.kernel.org>
Reply-to: <cve@kernel.org>, <linux-kernel@vger.kernel.org>
Subject: CVE-2026-23086: vsock/virtio: cap TX credit to local buffer size
Message-Id: <2026020423-CVE-2026-23086-9ad9@gregkh>
Content-Length: 4855
Lines: 114
X-Developer-Signature: v=1; a=openpgp-sha256; l=4970;
 i=gregkh@linuxfoundation.org; h=from:subject:message-id;
 bh=VyNKzmDaJ4F8iV27EGuaG06suTyDv0TuA9oCG899mqE=;
 b=owGbwMvMwCRo6H6F97bub03G02pJDJnNBfH665iUzWYrKa96MXGvwfpHGaerjgqdspKqOM4do
 XmBjXdTRywLgyATg6yYIsuXbTxH91ccUvQytD0NM4eVCWQIAxenAEwkrJhhroh5AU+4ZtU/bZaV
 KkyC7/9cZk7fwjDPeJbKy0LpJvUtxTVpp9rS51WXHTkKAA==
X-Developer-Key: i=gregkh@linuxfoundation.org; a=openpgp;
 fpr=F4B60CC5BF78C2214A313DCB3147D40DDB2DFB29

Description
===========

In the Linux kernel, the following vulnerability has been resolved:

vsock/virtio: cap TX credit to local buffer size

The virtio transports derives its TX credit directly from peer_buf_alloc,
which is set from the remote endpoint's SO_VM_SOCKETS_BUFFER_SIZE value.

On the host side this means that the amount of data we are willing to
queue for a connection is scaled by a guest-chosen buffer size, rather
than the host's own vsock configuration. A malicious guest can advertise
a large buffer and read slowly, causing the host to allocate a
correspondingly large amount of sk_buff memory.
The same thing would happen in the guest with a malicious host, since
virtio transports share the same code base.

Introduce a small helper, virtio_transport_tx_buf_size(), that
returns min(peer_buf_alloc, buf_alloc), and use it wherever we consume
peer_buf_alloc.

This ensures the effective TX window is bounded by both the peer's
advertised buffer and our own buf_alloc (already clamped to
buffer_max_size via SO_VM_SOCKETS_BUFFER_MAX_SIZE), so a remote peer
cannot force the other to queue more data than allowed by its own
vsock settings.

On an unpatched Ubuntu 22.04 host (~64 GiB RAM), running a PoC with
32 guest vsock connections advertising 2 GiB each and reading slowly
drove Slab/SUnreclaim from ~0.5 GiB to ~57 GiB; the system only
recovered after killing the QEMU process. That said, if QEMU memory is
limited with cgroups, the maximum memory used will be limited.

With this patch applied:

  Before:
    MemFree:        ~61.6 GiB
    Slab:           ~142 MiB
    SUnreclaim:     ~117 MiB

  After 32 high-credit connections:
    MemFree:        ~61.5 GiB
    Slab:           ~178 MiB
    SUnreclaim:     ~152 MiB

Only ~35 MiB increase in Slab/SUnreclaim, no host OOM, and the guest
remains responsive.

Compatibility with non-virtio transports:

  - VMCI uses the AF_VSOCK buffer knobs to size its queue pairs per
    socket based on the local vsk->buffer_* values; the remote side
    cannot enlarge those queues beyond what the local endpoint
    configured.

  - Hyper-V's vsock transport uses fixed-size VMBus ring buffers and
    an MTU bound; there is no peer-controlled credit field comparable
    to peer_buf_alloc, and the remote endpoint cannot drive in-flight
    kernel memory above those ring sizes.

  - The loopback path reuses virtio_transport_common.c, so it
    naturally follows the same semantics as the virtio transport.

This change is limited to virtio_transport_common.c and thus affects
virtio-vsock, vhost-vsock, and loopback, bringing them in line with the
"remote window intersected with local policy" behaviour that VMCI and
Hyper-V already effectively have.

[Stefano: small adjustments after changing the previous patch]
[Stefano: tweak the commit message]

The Linux kernel CVE team has assigned CVE-2026-23086 to this issue.


Affected and fixed versions
===========================

	Issue introduced in 4.8 with commit 06a8fc78367d070720af960dcecec917d3ae5f3b and fixed in 6.6.122 with commit d9d5f222558b42f6277eafaaa6080966faf37676
	Issue introduced in 4.8 with commit 06a8fc78367d070720af960dcecec917d3ae5f3b and fixed in 6.12.68 with commit c0e42fb0e054c2b2ec4ee80f48ccd256ae0227ce
	Issue introduced in 4.8 with commit 06a8fc78367d070720af960dcecec917d3ae5f3b and fixed in 6.18.8 with commit 84ef86aa7120449828d1e0ce438c499014839711
	Issue introduced in 4.8 with commit 06a8fc78367d070720af960dcecec917d3ae5f3b and fixed in 6.19-rc7 with commit 8ee784fdf006cbe8739cfa093f54d326cbf54037

Please see https://www.kernel.org for a full list of currently supported
kernel versions by the kernel community.

Unaffected versions might change over time as fixes are backported to
older supported kernel versions.  The official CVE entry at
	https://cve.org/CVERecord/?id=CVE-2026-23086
will be updated if fixes are backported, please check that for the most
up to date information about this issue.


Affected files
==============

The file(s) affected by this issue are:
	net/vmw_vsock/virtio_transport_common.c


Mitigation
==========

The Linux kernel CVE team recommends that you update to the latest
stable kernel version for this, and many other bugfixes.  Individual
changes are never tested alone, but rather are part of a larger kernel
release.  Cherry-picking individual commits is not recommended or
supported by the Linux kernel community at all.  If however, updating to
the latest release is impossible, the individual changes to resolve this
issue can be found at these commits:
	https://git.kernel.org/stable/c/d9d5f222558b42f6277eafaaa6080966faf37676
	https://git.kernel.org/stable/c/c0e42fb0e054c2b2ec4ee80f48ccd256ae0227ce
	https://git.kernel.org/stable/c/84ef86aa7120449828d1e0ce438c499014839711
	https://git.kernel.org/stable/c/8ee784fdf006cbe8739cfa093f54d326cbf54037
